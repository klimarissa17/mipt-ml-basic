{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Lab2_DL_part3_poetry.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6BF85vFRxtz"
      },
      "source": [
        "## Lab 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrXVdDtBRxt6"
      },
      "source": [
        "### Part 3. Poetry generation\n",
        "\n",
        "Let's try to generate some poetry using RNNs. \n",
        "\n",
        "You have several choices here: \n",
        "\n",
        "* The Shakespeare sonnets, file `sonnets.txt` available in the notebook directory.\n",
        "\n",
        "* Роман в стихах \"Евгений Онегин\" Александра Сергеевича Пушкина. В предобработанном виде доступен по [ссылке](https://github.com/attatrol/data_sources/blob/master/onegin.txt).\n",
        "\n",
        "* Some other text source, if it will be approved by the course staff.\n",
        "\n",
        "Text generation can be designed in several steps:\n",
        "    \n",
        "1. Data loading.\n",
        "2. Dictionary generation.\n",
        "3. Data preprocessing.\n",
        "4. Model (neural network) training.\n",
        "5. Text generation (model evaluation).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN7x27G5Rxt7"
      },
      "source": [
        "import string\n",
        "import os\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr4rnQ6tRxt8"
      },
      "source": [
        "### Data loading: Shakespeare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buBDL2NyRxt8"
      },
      "source": [
        "Shakespeare sonnets are awailable at this [link](http://www.gutenberg.org/ebooks/1041?msg=welcome_stranger). In addition, they are stored in the same directory as this notebook (`sonnetes.txt`). Simple preprocessing is already done for you in the next cell: all technical info is dropped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "CcflgmayRxt9"
      },
      "source": [
        "if not os.path.exists('sonnets.txt'):\n",
        "    !wget https://raw.githubusercontent.com/girafe-ai/ml-mipt/master/homeworks_basic/Lab2_DL/sonnets.txt\n",
        "\n",
        "with open('sonnets.txt', 'r') as iofile:\n",
        "    text = iofile.readlines()\n",
        "    \n",
        "TEXT_START = 45\n",
        "TEXT_END = -368\n",
        "text = text[TEXT_START : TEXT_END]\n",
        "assert len(text) == 2616"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uEPH-QNRxt9"
      },
      "source": [
        "In opposite to the in-class practice, this time we want to predict complex text. Let's reduce the complexity of the task and lowercase all the symbols.\n",
        "\n",
        "Now variable `text` is a list of strings. Join all the strings into one and lowercase it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ot_4oBoURxt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3750c30f-fd25-47f2-9332-c45807693de1"
      },
      "source": [
        "# Join all the strings into one and lowercase it\n",
        "# Put result into variable text.\n",
        "\n",
        "# Your great code here\n",
        "text = ''.join(text).lower()\n",
        "assert len(text) == 100225, 'Are you sure you have concatenated all the strings?'\n",
        "assert not any([x in set(text) for x in string.ascii_uppercase]), 'Uppercase letters are present'\n",
        "print('OK!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OK!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar5exZrORxt-"
      },
      "source": [
        "### Data loading: \"Евгений Онегин\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwEobvfjRxt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad3f3c0b-1555-4552-9eda-d0c78442b10a"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/attatrol/data_sources/master/onegin.txt\n",
        "    \n",
        "with open('onegin.txt', 'r') as iofile:\n",
        "    text = iofile.readlines()\n",
        "    \n",
        "text = [x.replace('\\t\\t', '') for x in text]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-11 20:53:10--  https://raw.githubusercontent.com/attatrol/data_sources/master/onegin.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 262521 (256K) [text/plain]\n",
            "Saving to: ‘onegin.txt’\n",
            "\n",
            "\ronegin.txt            0%[                    ]       0  --.-KB/s               \ronegin.txt          100%[===================>] 256.37K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2021-06-11 20:53:11 (44.3 MB/s) - ‘onegin.txt’ saved [262521/262521]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WuogzsfRxt_"
      },
      "source": [
        "In opposite to the in-class practice, this time we want to predict complex text. Let's reduce the complexity of the task and lowercase all the symbols.\n",
        "\n",
        "Now variable `text` is a list of strings. Join all the strings into one and lowercase it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEFg64VeRxuA"
      },
      "source": [
        "# Join all the strings into one and lowercase it\n",
        "# Put result into variable text.\n",
        "\n",
        "# Your great code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naIcBoWG4O-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87b07ae6-e20a-4217-8f05-d0db9f5165dc"
      },
      "source": [
        "type(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeQyBrpsRxuA"
      },
      "source": [
        "Put all the characters, that you've seen in the text, into variable `tokens`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo1a087dRxuA"
      },
      "source": [
        "tokens = sorted(set(text + '$@&'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bL_angmRxuA"
      },
      "source": [
        "Create dictionary `token_to_idx = {<char>: <index>}` and dictionary `idx_to_token = {<index>: <char>}`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "EY8F-vSARxuB"
      },
      "source": [
        "token_to_idx = {token: idx for idx, token in enumerate(tokens)}\n",
        "idx_to_token = {idx: token for idx, token in enumerate(tokens)}\n",
        "assert len(token_to_idx) == len(idx_to_token)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVY2Eg-PRxuB"
      },
      "source": [
        "*Comment: in this task we have only 38 different tokens, so let's use one-hot encoding.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oFnai8TRxuB"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHXjds6pRxuC"
      },
      "source": [
        "Now we want to build and train recurrent neural net which would be able to something similar to Shakespeare's poetry.\n",
        "\n",
        "Let's use vanilla RNN, similar to the one created during the lesson."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIgS6PqbHYds"
      },
      "source": [
        "def to_matrix(pieces, max_len=None):\n",
        "  if max_len == None: \n",
        "    max_len = max(map(len, pieces))\n",
        "  pad = token_to_idx['@']\n",
        "  sos = token_to_idx['$']\n",
        "  eos = token_to_idx['&']\n",
        "\n",
        "  text_ix = np.zeros((len(pieces), max_len), dtype='int32') + pad\n",
        "  #text_ix[:0] += sos\n",
        "\n",
        "  for i in range(len(pieces)): \n",
        "    line_ix = [token_to_idx[c] for c in pieces[i]]\n",
        "    #line_ix.append(eos)\n",
        "    text_ix[i, :len(line_ix)] = line_ix\n",
        "    \n",
        "  return text_ix"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldTQyIgc3nT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "336de69c-4141-46ce-ea40-f1e9b6695692"
      },
      "source": [
        "tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '$',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " '@',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ahf2ukVNAdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef494210-1fed-4cc7-c208-911ae7642d4a"
      },
      "source": [
        "names = ['olga', 'maxim', 'catherine', 'mary']\n",
        "to_matrix(names)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[29, 26, 21, 15, 14, 14, 14, 14, 14],\n",
              "       [27, 15, 38, 23, 27, 14, 14, 14, 14],\n",
              "       [17, 15, 34, 22, 19, 32, 23, 28, 19],\n",
              "       [27, 15, 32, 39, 14, 14, 14, 14, 14]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkegAyVMUJg7"
      },
      "source": [
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5NGdMn3UMyY"
      },
      "source": [
        "class CharRNNCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement the scheme above as torch module\n",
        "    \"\"\"\n",
        "    def __init__(self, num_tokens=len(tokens), embedding_size=16, rnn_num_units=64):\n",
        "        super(self.__class__,self).__init__()\n",
        "        self.num_units = rnn_num_units\n",
        "        \n",
        "        self.embedding = nn.Embedding(num_tokens, embedding_size)\n",
        "        self.rnn_update = nn.Linear(embedding_size + rnn_num_units, rnn_num_units)\n",
        "        self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
        "        \n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"\n",
        "        This method computes h_next(x, h_prev) and log P(x_next | h_next)\n",
        "        We'll call it repeatedly to produce the whole sequence.\n",
        "        \n",
        "        :param x: batch of character ids, containing vector of int64\n",
        "        :param h_prev: previous rnn hidden states, containing matrix [batch, rnn_num_units] of float32\n",
        "        \"\"\"\n",
        "        # get vector embedding of x\n",
        "        x_emb = self.embedding(x)\n",
        "        \n",
        "        # compute next hidden state using self.rnn_update\n",
        "        # hint: use torch.cat(..., dim=...) for concatenation\n",
        "        x_and_h = torch.cat([x_emb, h_prev], dim=1)\n",
        "        h_next = self.rnn_update(x_and_h)\n",
        "        \n",
        "        h_next = torch.tanh(h_next)\n",
        "        \n",
        "        assert h_next.size() == h_prev.size()\n",
        "        \n",
        "        #compute logits for next character probs\n",
        "        logits = self.rnn_to_logits(h_next)\n",
        "        \n",
        "        return h_next, F.log_softmax(logits, -1)\n",
        "    \n",
        "    def initial_state(self, batch_size):\n",
        "        \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n",
        "        return torch.zeros(batch_size, self.num_units, requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVPMxKkNRIS1"
      },
      "source": [
        "char_rnn = CharRNNCell()\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVTRQ5hSRpHs"
      },
      "source": [
        "def rnn_loop(char_rnn, batch_ix):\n",
        "    \"\"\"\n",
        "    Computes log P(next_character) for all time-steps in names_ix\n",
        "    :param names_ix: an int32 matrix of shape [batch, time], output of to_matrix(names)\n",
        "    \"\"\"\n",
        "    batch_size, max_length = batch_ix.size()\n",
        "    hid_state = char_rnn.initial_state(batch_size)\n",
        "    logprobs = []\n",
        "\n",
        "    for x_t in batch_ix.transpose(0,1):\n",
        "        hid_state, logp_next = char_rnn(x_t, hid_state)  # <-- here we call your one-step code\n",
        "        logprobs.append(logp_next)\n",
        "        \n",
        "    return torch.stack(logprobs, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjXNYyNY4uXI"
      },
      "source": [
        "pieces = [text[i:i+30] for i in range(300)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vPrmcNy47M1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f68dd93e-d339-44c1-f5de-d486923e0f31"
      },
      "source": [
        "pieces"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  from fairest creatures we de',\n",
              " ' from fairest creatures we des',\n",
              " 'from fairest creatures we desi',\n",
              " 'rom fairest creatures we desir',\n",
              " 'om fairest creatures we desire',\n",
              " 'm fairest creatures we desire ',\n",
              " ' fairest creatures we desire i',\n",
              " 'fairest creatures we desire in',\n",
              " 'airest creatures we desire inc',\n",
              " 'irest creatures we desire incr',\n",
              " 'rest creatures we desire incre',\n",
              " 'est creatures we desire increa',\n",
              " 'st creatures we desire increas',\n",
              " 't creatures we desire increase',\n",
              " ' creatures we desire increase,',\n",
              " 'creatures we desire increase,\\n',\n",
              " 'reatures we desire increase,\\n ',\n",
              " 'eatures we desire increase,\\n  ',\n",
              " 'atures we desire increase,\\n  t',\n",
              " 'tures we desire increase,\\n  th',\n",
              " 'ures we desire increase,\\n  tha',\n",
              " 'res we desire increase,\\n  that',\n",
              " 'es we desire increase,\\n  that ',\n",
              " 's we desire increase,\\n  that t',\n",
              " ' we desire increase,\\n  that th',\n",
              " 'we desire increase,\\n  that the',\n",
              " 'e desire increase,\\n  that ther',\n",
              " ' desire increase,\\n  that there',\n",
              " 'desire increase,\\n  that thereb',\n",
              " 'esire increase,\\n  that thereby',\n",
              " 'sire increase,\\n  that thereby ',\n",
              " 'ire increase,\\n  that thereby b',\n",
              " 're increase,\\n  that thereby be',\n",
              " 'e increase,\\n  that thereby bea',\n",
              " ' increase,\\n  that thereby beau',\n",
              " 'increase,\\n  that thereby beaut',\n",
              " 'ncrease,\\n  that thereby beauty',\n",
              " \"crease,\\n  that thereby beauty'\",\n",
              " \"rease,\\n  that thereby beauty's\",\n",
              " \"ease,\\n  that thereby beauty's \",\n",
              " \"ase,\\n  that thereby beauty's r\",\n",
              " \"se,\\n  that thereby beauty's ro\",\n",
              " \"e,\\n  that thereby beauty's ros\",\n",
              " \",\\n  that thereby beauty's rose\",\n",
              " \"\\n  that thereby beauty's rose \",\n",
              " \"  that thereby beauty's rose m\",\n",
              " \" that thereby beauty's rose mi\",\n",
              " \"that thereby beauty's rose mig\",\n",
              " \"hat thereby beauty's rose migh\",\n",
              " \"at thereby beauty's rose might\",\n",
              " \"t thereby beauty's rose might \",\n",
              " \" thereby beauty's rose might n\",\n",
              " \"thereby beauty's rose might ne\",\n",
              " \"hereby beauty's rose might nev\",\n",
              " \"ereby beauty's rose might neve\",\n",
              " \"reby beauty's rose might never\",\n",
              " \"eby beauty's rose might never \",\n",
              " \"by beauty's rose might never d\",\n",
              " \"y beauty's rose might never di\",\n",
              " \" beauty's rose might never die\",\n",
              " \"beauty's rose might never die,\",\n",
              " \"eauty's rose might never die,\\n\",\n",
              " \"auty's rose might never die,\\n \",\n",
              " \"uty's rose might never die,\\n  \",\n",
              " \"ty's rose might never die,\\n  b\",\n",
              " \"y's rose might never die,\\n  bu\",\n",
              " \"'s rose might never die,\\n  but\",\n",
              " 's rose might never die,\\n  but ',\n",
              " ' rose might never die,\\n  but a',\n",
              " 'rose might never die,\\n  but as',\n",
              " 'ose might never die,\\n  but as ',\n",
              " 'se might never die,\\n  but as t',\n",
              " 'e might never die,\\n  but as th',\n",
              " ' might never die,\\n  but as the',\n",
              " 'might never die,\\n  but as the ',\n",
              " 'ight never die,\\n  but as the r',\n",
              " 'ght never die,\\n  but as the ri',\n",
              " 'ht never die,\\n  but as the rip',\n",
              " 't never die,\\n  but as the ripe',\n",
              " ' never die,\\n  but as the riper',\n",
              " 'never die,\\n  but as the riper ',\n",
              " 'ever die,\\n  but as the riper s',\n",
              " 'ver die,\\n  but as the riper sh',\n",
              " 'er die,\\n  but as the riper sho',\n",
              " 'r die,\\n  but as the riper shou',\n",
              " ' die,\\n  but as the riper shoul',\n",
              " 'die,\\n  but as the riper should',\n",
              " 'ie,\\n  but as the riper should ',\n",
              " 'e,\\n  but as the riper should b',\n",
              " ',\\n  but as the riper should by',\n",
              " '\\n  but as the riper should by ',\n",
              " '  but as the riper should by t',\n",
              " ' but as the riper should by ti',\n",
              " 'but as the riper should by tim',\n",
              " 'ut as the riper should by time',\n",
              " 't as the riper should by time ',\n",
              " ' as the riper should by time d',\n",
              " 'as the riper should by time de',\n",
              " 's the riper should by time dec',\n",
              " ' the riper should by time dece',\n",
              " 'the riper should by time decea',\n",
              " 'he riper should by time deceas',\n",
              " 'e riper should by time decease',\n",
              " ' riper should by time decease,',\n",
              " 'riper should by time decease,\\n',\n",
              " 'iper should by time decease,\\n ',\n",
              " 'per should by time decease,\\n  ',\n",
              " 'er should by time decease,\\n  h',\n",
              " 'r should by time decease,\\n  hi',\n",
              " ' should by time decease,\\n  his',\n",
              " 'should by time decease,\\n  his ',\n",
              " 'hould by time decease,\\n  his t',\n",
              " 'ould by time decease,\\n  his te',\n",
              " 'uld by time decease,\\n  his ten',\n",
              " 'ld by time decease,\\n  his tend',\n",
              " 'd by time decease,\\n  his tende',\n",
              " ' by time decease,\\n  his tender',\n",
              " 'by time decease,\\n  his tender ',\n",
              " 'y time decease,\\n  his tender h',\n",
              " ' time decease,\\n  his tender he',\n",
              " 'time decease,\\n  his tender hei',\n",
              " 'ime decease,\\n  his tender heir',\n",
              " 'me decease,\\n  his tender heir ',\n",
              " 'e decease,\\n  his tender heir m',\n",
              " ' decease,\\n  his tender heir mi',\n",
              " 'decease,\\n  his tender heir mig',\n",
              " 'ecease,\\n  his tender heir migh',\n",
              " 'cease,\\n  his tender heir might',\n",
              " 'ease,\\n  his tender heir might ',\n",
              " 'ase,\\n  his tender heir might b',\n",
              " 'se,\\n  his tender heir might be',\n",
              " 'e,\\n  his tender heir might bea',\n",
              " ',\\n  his tender heir might bear',\n",
              " '\\n  his tender heir might bear ',\n",
              " '  his tender heir might bear h',\n",
              " ' his tender heir might bear hi',\n",
              " 'his tender heir might bear his',\n",
              " 'is tender heir might bear his ',\n",
              " 's tender heir might bear his m',\n",
              " ' tender heir might bear his me',\n",
              " 'tender heir might bear his mem',\n",
              " 'ender heir might bear his memo',\n",
              " 'nder heir might bear his memor',\n",
              " 'der heir might bear his memory',\n",
              " 'er heir might bear his memory:',\n",
              " 'r heir might bear his memory:\\n',\n",
              " ' heir might bear his memory:\\n ',\n",
              " 'heir might bear his memory:\\n  ',\n",
              " 'eir might bear his memory:\\n  b',\n",
              " 'ir might bear his memory:\\n  bu',\n",
              " 'r might bear his memory:\\n  but',\n",
              " ' might bear his memory:\\n  but ',\n",
              " 'might bear his memory:\\n  but t',\n",
              " 'ight bear his memory:\\n  but th',\n",
              " 'ght bear his memory:\\n  but tho',\n",
              " 'ht bear his memory:\\n  but thou',\n",
              " 't bear his memory:\\n  but thou,',\n",
              " ' bear his memory:\\n  but thou, ',\n",
              " 'bear his memory:\\n  but thou, c',\n",
              " 'ear his memory:\\n  but thou, co',\n",
              " 'ar his memory:\\n  but thou, con',\n",
              " 'r his memory:\\n  but thou, cont',\n",
              " ' his memory:\\n  but thou, contr',\n",
              " 'his memory:\\n  but thou, contra',\n",
              " 'is memory:\\n  but thou, contrac',\n",
              " 's memory:\\n  but thou, contract',\n",
              " ' memory:\\n  but thou, contracte',\n",
              " 'memory:\\n  but thou, contracted',\n",
              " 'emory:\\n  but thou, contracted ',\n",
              " 'mory:\\n  but thou, contracted t',\n",
              " 'ory:\\n  but thou, contracted to',\n",
              " 'ry:\\n  but thou, contracted to ',\n",
              " 'y:\\n  but thou, contracted to t',\n",
              " ':\\n  but thou, contracted to th',\n",
              " '\\n  but thou, contracted to thi',\n",
              " '  but thou, contracted to thin',\n",
              " ' but thou, contracted to thine',\n",
              " 'but thou, contracted to thine ',\n",
              " 'ut thou, contracted to thine o',\n",
              " 't thou, contracted to thine ow',\n",
              " ' thou, contracted to thine own',\n",
              " 'thou, contracted to thine own ',\n",
              " 'hou, contracted to thine own b',\n",
              " 'ou, contracted to thine own br',\n",
              " 'u, contracted to thine own bri',\n",
              " ', contracted to thine own brig',\n",
              " ' contracted to thine own brigh',\n",
              " 'contracted to thine own bright',\n",
              " 'ontracted to thine own bright ',\n",
              " 'ntracted to thine own bright e',\n",
              " 'tracted to thine own bright ey',\n",
              " 'racted to thine own bright eye',\n",
              " 'acted to thine own bright eyes',\n",
              " 'cted to thine own bright eyes,',\n",
              " 'ted to thine own bright eyes,\\n',\n",
              " 'ed to thine own bright eyes,\\n ',\n",
              " 'd to thine own bright eyes,\\n  ',\n",
              " ' to thine own bright eyes,\\n  f',\n",
              " 'to thine own bright eyes,\\n  fe',\n",
              " 'o thine own bright eyes,\\n  fee',\n",
              " ' thine own bright eyes,\\n  feed',\n",
              " \"thine own bright eyes,\\n  feed'\",\n",
              " \"hine own bright eyes,\\n  feed's\",\n",
              " \"ine own bright eyes,\\n  feed'st\",\n",
              " \"ne own bright eyes,\\n  feed'st \",\n",
              " \"e own bright eyes,\\n  feed'st t\",\n",
              " \" own bright eyes,\\n  feed'st th\",\n",
              " \"own bright eyes,\\n  feed'st thy\",\n",
              " \"wn bright eyes,\\n  feed'st thy \",\n",
              " \"n bright eyes,\\n  feed'st thy l\",\n",
              " \" bright eyes,\\n  feed'st thy li\",\n",
              " \"bright eyes,\\n  feed'st thy lig\",\n",
              " \"right eyes,\\n  feed'st thy ligh\",\n",
              " \"ight eyes,\\n  feed'st thy light\",\n",
              " \"ght eyes,\\n  feed'st thy light'\",\n",
              " \"ht eyes,\\n  feed'st thy light's\",\n",
              " \"t eyes,\\n  feed'st thy light's \",\n",
              " \" eyes,\\n  feed'st thy light's f\",\n",
              " \"eyes,\\n  feed'st thy light's fl\",\n",
              " \"yes,\\n  feed'st thy light's fla\",\n",
              " \"es,\\n  feed'st thy light's flam\",\n",
              " \"s,\\n  feed'st thy light's flame\",\n",
              " \",\\n  feed'st thy light's flame \",\n",
              " \"\\n  feed'st thy light's flame w\",\n",
              " \"  feed'st thy light's flame wi\",\n",
              " \" feed'st thy light's flame wit\",\n",
              " \"feed'st thy light's flame with\",\n",
              " \"eed'st thy light's flame with \",\n",
              " \"ed'st thy light's flame with s\",\n",
              " \"d'st thy light's flame with se\",\n",
              " \"'st thy light's flame with sel\",\n",
              " \"st thy light's flame with self\",\n",
              " \"t thy light's flame with self-\",\n",
              " \" thy light's flame with self-s\",\n",
              " \"thy light's flame with self-su\",\n",
              " \"hy light's flame with self-sub\",\n",
              " \"y light's flame with self-subs\",\n",
              " \" light's flame with self-subst\",\n",
              " \"light's flame with self-substa\",\n",
              " \"ight's flame with self-substan\",\n",
              " \"ght's flame with self-substant\",\n",
              " \"ht's flame with self-substanti\",\n",
              " \"t's flame with self-substantia\",\n",
              " \"'s flame with self-substantial\",\n",
              " 's flame with self-substantial ',\n",
              " ' flame with self-substantial f',\n",
              " 'flame with self-substantial fu',\n",
              " 'lame with self-substantial fue',\n",
              " 'ame with self-substantial fuel',\n",
              " 'me with self-substantial fuel,',\n",
              " 'e with self-substantial fuel,\\n',\n",
              " ' with self-substantial fuel,\\n ',\n",
              " 'with self-substantial fuel,\\n  ',\n",
              " 'ith self-substantial fuel,\\n  m',\n",
              " 'th self-substantial fuel,\\n  ma',\n",
              " 'h self-substantial fuel,\\n  mak',\n",
              " ' self-substantial fuel,\\n  maki',\n",
              " 'self-substantial fuel,\\n  makin',\n",
              " 'elf-substantial fuel,\\n  making',\n",
              " 'lf-substantial fuel,\\n  making ',\n",
              " 'f-substantial fuel,\\n  making a',\n",
              " '-substantial fuel,\\n  making a ',\n",
              " 'substantial fuel,\\n  making a f',\n",
              " 'ubstantial fuel,\\n  making a fa',\n",
              " 'bstantial fuel,\\n  making a fam',\n",
              " 'stantial fuel,\\n  making a fami',\n",
              " 'tantial fuel,\\n  making a famin',\n",
              " 'antial fuel,\\n  making a famine',\n",
              " 'ntial fuel,\\n  making a famine ',\n",
              " 'tial fuel,\\n  making a famine w',\n",
              " 'ial fuel,\\n  making a famine wh',\n",
              " 'al fuel,\\n  making a famine whe',\n",
              " 'l fuel,\\n  making a famine wher',\n",
              " ' fuel,\\n  making a famine where',\n",
              " 'fuel,\\n  making a famine where ',\n",
              " 'uel,\\n  making a famine where a',\n",
              " 'el,\\n  making a famine where ab',\n",
              " 'l,\\n  making a famine where abu',\n",
              " ',\\n  making a famine where abun',\n",
              " '\\n  making a famine where abund',\n",
              " '  making a famine where abunda',\n",
              " ' making a famine where abundan',\n",
              " 'making a famine where abundanc',\n",
              " 'aking a famine where abundance',\n",
              " 'king a famine where abundance ',\n",
              " 'ing a famine where abundance l',\n",
              " 'ng a famine where abundance li',\n",
              " 'g a famine where abundance lie',\n",
              " ' a famine where abundance lies',\n",
              " 'a famine where abundance lies,',\n",
              " ' famine where abundance lies,\\n',\n",
              " 'famine where abundance lies,\\n ',\n",
              " 'amine where abundance lies,\\n  ',\n",
              " 'mine where abundance lies,\\n  t',\n",
              " 'ine where abundance lies,\\n  th',\n",
              " 'ne where abundance lies,\\n  thy',\n",
              " 'e where abundance lies,\\n  thy ',\n",
              " ' where abundance lies,\\n  thy s',\n",
              " 'where abundance lies,\\n  thy se',\n",
              " 'here abundance lies,\\n  thy sel']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr9nEG1URuIr"
      },
      "source": [
        "batch_ix = to_matrix(pieces)\n",
        "batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "\n",
        "logp_seq = rnn_loop(char_rnn, batch_ix)\n",
        "\n",
        "assert torch.max(logp_seq).data.numpy() <= 0\n",
        "assert tuple(logp_seq.size()) ==  batch_ix.shape + (len(tokens),)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScbP9H295M-d"
      },
      "source": [
        "predictions_logp = logp_seq[:, :-1]\n",
        "actual_next_tokens = batch_ix[:, 1:]\n",
        "\n",
        "logp_next = torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:,:,None])\n",
        "\n",
        "loss = -logp_next.mean()\n",
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAr7WjxnC7DH"
      },
      "source": [
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xnHMG2eCoOV"
      },
      "source": [
        "def get_pieces(piece_len, num):\n",
        "  res = []\n",
        "  for i in range(num):\n",
        "    start = random.randrange(0, len(text) - piece_len)\n",
        "    res.append(text[start:start + piece_len])\n",
        "  return res\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdUfLydNDTnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1dd2f50-133d-4088-a4c5-f0e8c716b573"
      },
      "source": [
        "get_pieces(20, 5)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['t.\\n\\n  cxi\\n\\n  o! for ',\n",
              " 'he treasure of his s',\n",
              " 'n to my sportive blo',\n",
              " 's that to thee resor',\n",
              " 'on myself with prese']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv8la_TMB5dV"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from random import sample\n",
        "\n",
        "char_rnn = CharRNNCell()\n",
        "opt = torch.optim.Adam(char_rnn.parameters())\n",
        "history = []"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfQKdULJB_CE",
        "outputId": "556a5f62-d300-4189-b0b9-41817395b01c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "MAX_LEN = 200\n",
        "pieces = get_pieces(MAX_LEN, 1000)\n",
        "\n",
        "actual_next_tokens = batch_ix\n",
        "for i in range(1000):\n",
        "  samples = torch.tensor(to_matrix(pieces), dtype=torch.int64)\n",
        "  batch_ix = samples[:, :100]\n",
        "  opt.zero_grad()\n",
        "  logp_seq = rnn_loop(char_rnn, batch_ix) \n",
        "  actual_next_tokens = samples[:, 100:]\n",
        "  predictions_logp = logp_seq\n",
        "  logp_next = torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:,:,None])\n",
        "  loss = -logp_next.mean()\n",
        "  print(loss)\n",
        "  loss.backward()\n",
        "  opt.step()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(3.6253, grad_fn=<NegBackward>)\n",
            "tensor(3.6052, grad_fn=<NegBackward>)\n",
            "tensor(3.5821, grad_fn=<NegBackward>)\n",
            "tensor(3.5551, grad_fn=<NegBackward>)\n",
            "tensor(3.5234, grad_fn=<NegBackward>)\n",
            "tensor(3.4858, grad_fn=<NegBackward>)\n",
            "tensor(3.4416, grad_fn=<NegBackward>)\n",
            "tensor(3.3908, grad_fn=<NegBackward>)\n",
            "tensor(3.3349, grad_fn=<NegBackward>)\n",
            "tensor(3.2773, grad_fn=<NegBackward>)\n",
            "tensor(3.2225, grad_fn=<NegBackward>)\n",
            "tensor(3.1748, grad_fn=<NegBackward>)\n",
            "tensor(3.1359, grad_fn=<NegBackward>)\n",
            "tensor(3.1058, grad_fn=<NegBackward>)\n",
            "tensor(3.0829, grad_fn=<NegBackward>)\n",
            "tensor(3.0656, grad_fn=<NegBackward>)\n",
            "tensor(3.0526, grad_fn=<NegBackward>)\n",
            "tensor(3.0426, grad_fn=<NegBackward>)\n",
            "tensor(3.0349, grad_fn=<NegBackward>)\n",
            "tensor(3.0302, grad_fn=<NegBackward>)\n",
            "tensor(3.0279, grad_fn=<NegBackward>)\n",
            "tensor(3.0296, grad_fn=<NegBackward>)\n",
            "tensor(3.0251, grad_fn=<NegBackward>)\n",
            "tensor(3.0206, grad_fn=<NegBackward>)\n",
            "tensor(3.0162, grad_fn=<NegBackward>)\n",
            "tensor(3.0120, grad_fn=<NegBackward>)\n",
            "tensor(3.0081, grad_fn=<NegBackward>)\n",
            "tensor(3.0047, grad_fn=<NegBackward>)\n",
            "tensor(3.0018, grad_fn=<NegBackward>)\n",
            "tensor(2.9993, grad_fn=<NegBackward>)\n",
            "tensor(2.9973, grad_fn=<NegBackward>)\n",
            "tensor(2.9957, grad_fn=<NegBackward>)\n",
            "tensor(2.9944, grad_fn=<NegBackward>)\n",
            "tensor(2.9934, grad_fn=<NegBackward>)\n",
            "tensor(2.9925, grad_fn=<NegBackward>)\n",
            "tensor(2.9916, grad_fn=<NegBackward>)\n",
            "tensor(2.9884, grad_fn=<NegBackward>)\n",
            "tensor(2.9875, grad_fn=<NegBackward>)\n",
            "tensor(2.9867, grad_fn=<NegBackward>)\n",
            "tensor(2.9858, grad_fn=<NegBackward>)\n",
            "tensor(2.9824, grad_fn=<NegBackward>)\n",
            "tensor(2.9814, grad_fn=<NegBackward>)\n",
            "tensor(2.9806, grad_fn=<NegBackward>)\n",
            "tensor(2.9799, grad_fn=<NegBackward>)\n",
            "tensor(2.9793, grad_fn=<NegBackward>)\n",
            "tensor(2.9787, grad_fn=<NegBackward>)\n",
            "tensor(2.9782, grad_fn=<NegBackward>)\n",
            "tensor(2.9778, grad_fn=<NegBackward>)\n",
            "tensor(2.9774, grad_fn=<NegBackward>)\n",
            "tensor(2.9770, grad_fn=<NegBackward>)\n",
            "tensor(2.9744, grad_fn=<NegBackward>)\n",
            "tensor(2.9739, grad_fn=<NegBackward>)\n",
            "tensor(2.9735, grad_fn=<NegBackward>)\n",
            "tensor(2.9730, grad_fn=<NegBackward>)\n",
            "tensor(2.9701, grad_fn=<NegBackward>)\n",
            "tensor(2.9684, grad_fn=<NegBackward>)\n",
            "tensor(2.9680, grad_fn=<NegBackward>)\n",
            "tensor(2.9675, grad_fn=<NegBackward>)\n",
            "tensor(2.9653, grad_fn=<NegBackward>)\n",
            "tensor(2.9649, grad_fn=<NegBackward>)\n",
            "tensor(2.9647, grad_fn=<NegBackward>)\n",
            "tensor(2.9645, grad_fn=<NegBackward>)\n",
            "tensor(2.9643, grad_fn=<NegBackward>)\n",
            "tensor(2.9641, grad_fn=<NegBackward>)\n",
            "tensor(2.9639, grad_fn=<NegBackward>)\n",
            "tensor(2.9637, grad_fn=<NegBackward>)\n",
            "tensor(2.9635, grad_fn=<NegBackward>)\n",
            "tensor(2.9634, grad_fn=<NegBackward>)\n",
            "tensor(2.9632, grad_fn=<NegBackward>)\n",
            "tensor(2.9630, grad_fn=<NegBackward>)\n",
            "tensor(2.9629, grad_fn=<NegBackward>)\n",
            "tensor(2.9627, grad_fn=<NegBackward>)\n",
            "tensor(2.9626, grad_fn=<NegBackward>)\n",
            "tensor(2.9624, grad_fn=<NegBackward>)\n",
            "tensor(2.9623, grad_fn=<NegBackward>)\n",
            "tensor(2.9622, grad_fn=<NegBackward>)\n",
            "tensor(2.9620, grad_fn=<NegBackward>)\n",
            "tensor(2.9619, grad_fn=<NegBackward>)\n",
            "tensor(2.9618, grad_fn=<NegBackward>)\n",
            "tensor(2.9617, grad_fn=<NegBackward>)\n",
            "tensor(2.9615, grad_fn=<NegBackward>)\n",
            "tensor(2.9614, grad_fn=<NegBackward>)\n",
            "tensor(2.9613, grad_fn=<NegBackward>)\n",
            "tensor(2.9612, grad_fn=<NegBackward>)\n",
            "tensor(2.9611, grad_fn=<NegBackward>)\n",
            "tensor(2.9610, grad_fn=<NegBackward>)\n",
            "tensor(2.9608, grad_fn=<NegBackward>)\n",
            "tensor(2.9607, grad_fn=<NegBackward>)\n",
            "tensor(2.9606, grad_fn=<NegBackward>)\n",
            "tensor(2.9605, grad_fn=<NegBackward>)\n",
            "tensor(2.9604, grad_fn=<NegBackward>)\n",
            "tensor(2.9604, grad_fn=<NegBackward>)\n",
            "tensor(2.9603, grad_fn=<NegBackward>)\n",
            "tensor(2.9602, grad_fn=<NegBackward>)\n",
            "tensor(2.9601, grad_fn=<NegBackward>)\n",
            "tensor(2.9600, grad_fn=<NegBackward>)\n",
            "tensor(2.9599, grad_fn=<NegBackward>)\n",
            "tensor(2.9598, grad_fn=<NegBackward>)\n",
            "tensor(2.9598, grad_fn=<NegBackward>)\n",
            "tensor(2.9597, grad_fn=<NegBackward>)\n",
            "tensor(2.9596, grad_fn=<NegBackward>)\n",
            "tensor(2.9595, grad_fn=<NegBackward>)\n",
            "tensor(2.9594, grad_fn=<NegBackward>)\n",
            "tensor(2.9594, grad_fn=<NegBackward>)\n",
            "tensor(2.9593, grad_fn=<NegBackward>)\n",
            "tensor(2.9592, grad_fn=<NegBackward>)\n",
            "tensor(2.9592, grad_fn=<NegBackward>)\n",
            "tensor(2.9591, grad_fn=<NegBackward>)\n",
            "tensor(2.9590, grad_fn=<NegBackward>)\n",
            "tensor(2.9590, grad_fn=<NegBackward>)\n",
            "tensor(2.9589, grad_fn=<NegBackward>)\n",
            "tensor(2.9588, grad_fn=<NegBackward>)\n",
            "tensor(2.9588, grad_fn=<NegBackward>)\n",
            "tensor(2.9587, grad_fn=<NegBackward>)\n",
            "tensor(2.9586, grad_fn=<NegBackward>)\n",
            "tensor(2.9586, grad_fn=<NegBackward>)\n",
            "tensor(2.9585, grad_fn=<NegBackward>)\n",
            "tensor(2.9585, grad_fn=<NegBackward>)\n",
            "tensor(2.9584, grad_fn=<NegBackward>)\n",
            "tensor(2.9584, grad_fn=<NegBackward>)\n",
            "tensor(2.9583, grad_fn=<NegBackward>)\n",
            "tensor(2.9582, grad_fn=<NegBackward>)\n",
            "tensor(2.9582, grad_fn=<NegBackward>)\n",
            "tensor(2.9581, grad_fn=<NegBackward>)\n",
            "tensor(2.9581, grad_fn=<NegBackward>)\n",
            "tensor(2.9580, grad_fn=<NegBackward>)\n",
            "tensor(2.9580, grad_fn=<NegBackward>)\n",
            "tensor(2.9579, grad_fn=<NegBackward>)\n",
            "tensor(2.9579, grad_fn=<NegBackward>)\n",
            "tensor(2.9578, grad_fn=<NegBackward>)\n",
            "tensor(2.9578, grad_fn=<NegBackward>)\n",
            "tensor(2.9577, grad_fn=<NegBackward>)\n",
            "tensor(2.9577, grad_fn=<NegBackward>)\n",
            "tensor(2.9576, grad_fn=<NegBackward>)\n",
            "tensor(2.9576, grad_fn=<NegBackward>)\n",
            "tensor(2.9575, grad_fn=<NegBackward>)\n",
            "tensor(2.9575, grad_fn=<NegBackward>)\n",
            "tensor(2.9574, grad_fn=<NegBackward>)\n",
            "tensor(2.9574, grad_fn=<NegBackward>)\n",
            "tensor(2.9573, grad_fn=<NegBackward>)\n",
            "tensor(2.9573, grad_fn=<NegBackward>)\n",
            "tensor(2.9573, grad_fn=<NegBackward>)\n",
            "tensor(2.9572, grad_fn=<NegBackward>)\n",
            "tensor(2.9572, grad_fn=<NegBackward>)\n",
            "tensor(2.9571, grad_fn=<NegBackward>)\n",
            "tensor(2.9571, grad_fn=<NegBackward>)\n",
            "tensor(2.9570, grad_fn=<NegBackward>)\n",
            "tensor(2.9570, grad_fn=<NegBackward>)\n",
            "tensor(2.9570, grad_fn=<NegBackward>)\n",
            "tensor(2.9569, grad_fn=<NegBackward>)\n",
            "tensor(2.9569, grad_fn=<NegBackward>)\n",
            "tensor(2.9568, grad_fn=<NegBackward>)\n",
            "tensor(2.9568, grad_fn=<NegBackward>)\n",
            "tensor(2.9567, grad_fn=<NegBackward>)\n",
            "tensor(2.9567, grad_fn=<NegBackward>)\n",
            "tensor(2.9567, grad_fn=<NegBackward>)\n",
            "tensor(2.9566, grad_fn=<NegBackward>)\n",
            "tensor(2.9566, grad_fn=<NegBackward>)\n",
            "tensor(2.9566, grad_fn=<NegBackward>)\n",
            "tensor(2.9565, grad_fn=<NegBackward>)\n",
            "tensor(2.9565, grad_fn=<NegBackward>)\n",
            "tensor(2.9564, grad_fn=<NegBackward>)\n",
            "tensor(2.9564, grad_fn=<NegBackward>)\n",
            "tensor(2.9564, grad_fn=<NegBackward>)\n",
            "tensor(2.9563, grad_fn=<NegBackward>)\n",
            "tensor(2.9563, grad_fn=<NegBackward>)\n",
            "tensor(2.9562, grad_fn=<NegBackward>)\n",
            "tensor(2.9562, grad_fn=<NegBackward>)\n",
            "tensor(2.9562, grad_fn=<NegBackward>)\n",
            "tensor(2.9561, grad_fn=<NegBackward>)\n",
            "tensor(2.9561, grad_fn=<NegBackward>)\n",
            "tensor(2.9561, grad_fn=<NegBackward>)\n",
            "tensor(2.9560, grad_fn=<NegBackward>)\n",
            "tensor(2.9560, grad_fn=<NegBackward>)\n",
            "tensor(2.9560, grad_fn=<NegBackward>)\n",
            "tensor(2.9559, grad_fn=<NegBackward>)\n",
            "tensor(2.9559, grad_fn=<NegBackward>)\n",
            "tensor(2.9559, grad_fn=<NegBackward>)\n",
            "tensor(2.9558, grad_fn=<NegBackward>)\n",
            "tensor(2.9558, grad_fn=<NegBackward>)\n",
            "tensor(2.9558, grad_fn=<NegBackward>)\n",
            "tensor(2.9557, grad_fn=<NegBackward>)\n",
            "tensor(2.9557, grad_fn=<NegBackward>)\n",
            "tensor(2.9557, grad_fn=<NegBackward>)\n",
            "tensor(2.9556, grad_fn=<NegBackward>)\n",
            "tensor(2.9556, grad_fn=<NegBackward>)\n",
            "tensor(2.9556, grad_fn=<NegBackward>)\n",
            "tensor(2.9555, grad_fn=<NegBackward>)\n",
            "tensor(2.9555, grad_fn=<NegBackward>)\n",
            "tensor(2.9555, grad_fn=<NegBackward>)\n",
            "tensor(2.9554, grad_fn=<NegBackward>)\n",
            "tensor(2.9554, grad_fn=<NegBackward>)\n",
            "tensor(2.9554, grad_fn=<NegBackward>)\n",
            "tensor(2.9553, grad_fn=<NegBackward>)\n",
            "tensor(2.9553, grad_fn=<NegBackward>)\n",
            "tensor(2.9553, grad_fn=<NegBackward>)\n",
            "tensor(2.9552, grad_fn=<NegBackward>)\n",
            "tensor(2.9552, grad_fn=<NegBackward>)\n",
            "tensor(2.9552, grad_fn=<NegBackward>)\n",
            "tensor(2.9551, grad_fn=<NegBackward>)\n",
            "tensor(2.9551, grad_fn=<NegBackward>)\n",
            "tensor(2.9551, grad_fn=<NegBackward>)\n",
            "tensor(2.9551, grad_fn=<NegBackward>)\n",
            "tensor(2.9550, grad_fn=<NegBackward>)\n",
            "tensor(2.9550, grad_fn=<NegBackward>)\n",
            "tensor(2.9550, grad_fn=<NegBackward>)\n",
            "tensor(2.9549, grad_fn=<NegBackward>)\n",
            "tensor(2.9549, grad_fn=<NegBackward>)\n",
            "tensor(2.9549, grad_fn=<NegBackward>)\n",
            "tensor(2.9549, grad_fn=<NegBackward>)\n",
            "tensor(2.9548, grad_fn=<NegBackward>)\n",
            "tensor(2.9548, grad_fn=<NegBackward>)\n",
            "tensor(2.9548, grad_fn=<NegBackward>)\n",
            "tensor(2.9547, grad_fn=<NegBackward>)\n",
            "tensor(2.9547, grad_fn=<NegBackward>)\n",
            "tensor(2.9547, grad_fn=<NegBackward>)\n",
            "tensor(2.9547, grad_fn=<NegBackward>)\n",
            "tensor(2.9546, grad_fn=<NegBackward>)\n",
            "tensor(2.9546, grad_fn=<NegBackward>)\n",
            "tensor(2.9546, grad_fn=<NegBackward>)\n",
            "tensor(2.9545, grad_fn=<NegBackward>)\n",
            "tensor(2.9545, grad_fn=<NegBackward>)\n",
            "tensor(2.9545, grad_fn=<NegBackward>)\n",
            "tensor(2.9545, grad_fn=<NegBackward>)\n",
            "tensor(2.9544, grad_fn=<NegBackward>)\n",
            "tensor(2.9544, grad_fn=<NegBackward>)\n",
            "tensor(2.9544, grad_fn=<NegBackward>)\n",
            "tensor(2.9543, grad_fn=<NegBackward>)\n",
            "tensor(2.9543, grad_fn=<NegBackward>)\n",
            "tensor(2.9543, grad_fn=<NegBackward>)\n",
            "tensor(2.9543, grad_fn=<NegBackward>)\n",
            "tensor(2.9542, grad_fn=<NegBackward>)\n",
            "tensor(2.9542, grad_fn=<NegBackward>)\n",
            "tensor(2.9542, grad_fn=<NegBackward>)\n",
            "tensor(2.9542, grad_fn=<NegBackward>)\n",
            "tensor(2.9541, grad_fn=<NegBackward>)\n",
            "tensor(2.9541, grad_fn=<NegBackward>)\n",
            "tensor(2.9541, grad_fn=<NegBackward>)\n",
            "tensor(2.9541, grad_fn=<NegBackward>)\n",
            "tensor(2.9540, grad_fn=<NegBackward>)\n",
            "tensor(2.9540, grad_fn=<NegBackward>)\n",
            "tensor(2.9540, grad_fn=<NegBackward>)\n",
            "tensor(2.9540, grad_fn=<NegBackward>)\n",
            "tensor(2.9539, grad_fn=<NegBackward>)\n",
            "tensor(2.9539, grad_fn=<NegBackward>)\n",
            "tensor(2.9539, grad_fn=<NegBackward>)\n",
            "tensor(2.9538, grad_fn=<NegBackward>)\n",
            "tensor(2.9538, grad_fn=<NegBackward>)\n",
            "tensor(2.9538, grad_fn=<NegBackward>)\n",
            "tensor(2.9538, grad_fn=<NegBackward>)\n",
            "tensor(2.9537, grad_fn=<NegBackward>)\n",
            "tensor(2.9537, grad_fn=<NegBackward>)\n",
            "tensor(2.9537, grad_fn=<NegBackward>)\n",
            "tensor(2.9537, grad_fn=<NegBackward>)\n",
            "tensor(2.9536, grad_fn=<NegBackward>)\n",
            "tensor(2.9536, grad_fn=<NegBackward>)\n",
            "tensor(2.9536, grad_fn=<NegBackward>)\n",
            "tensor(2.9536, grad_fn=<NegBackward>)\n",
            "tensor(2.9535, grad_fn=<NegBackward>)\n",
            "tensor(2.9535, grad_fn=<NegBackward>)\n",
            "tensor(2.9535, grad_fn=<NegBackward>)\n",
            "tensor(2.9535, grad_fn=<NegBackward>)\n",
            "tensor(2.9534, grad_fn=<NegBackward>)\n",
            "tensor(2.9534, grad_fn=<NegBackward>)\n",
            "tensor(2.9534, grad_fn=<NegBackward>)\n",
            "tensor(2.9534, grad_fn=<NegBackward>)\n",
            "tensor(2.9533, grad_fn=<NegBackward>)\n",
            "tensor(2.9533, grad_fn=<NegBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDOBzyu0UAPb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imPsBe1bnnFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "346fe153-f96b-4b3a-af05-7608d783a29d"
      },
      "source": [
        "print(to_matrix(text[::100]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[14  1  4]\n",
            " [14 34  4]\n",
            " [14  1  4]\n",
            " ...\n",
            " [14 32  4]\n",
            " [14 23  4]\n",
            " [14 32  4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AyKFd19Tmzp"
      },
      "source": [
        "MAX_LENGTH = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCcRNb2MRxuC"
      },
      "source": [
        "Plot the loss function (axis X: number of epochs, axis Y: loss function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "1tmt7oDVRxuC"
      },
      "source": [
        "# Your plot code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SEZK4R9RxuD"
      },
      "source": [
        "def generate_sample(char_rnn, seed_phrase=' Hello', max_length=MAX_LENGTH, temperature=1.0):\n",
        "    '''\n",
        "    ### Disclaimer: this is an example function for text generation.\n",
        "    ### You can either adapt it in your code or create your own function\n",
        "    \n",
        "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
        "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
        "    :param max_length: maximum output length, including seed_phrase\n",
        "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs, \n",
        "        smaller temperature converges to the single most likely output.\n",
        "        \n",
        "    Be careful with the model output. This model waits logits (not probabilities/log-probabilities)\n",
        "    of the next symbol.\n",
        "    '''\n",
        "    \n",
        "    x_sequence = [token_to_id[token] for token in seed_phrase]\n",
        "    x_sequence = torch.tensor([[x_sequence]], dtype=torch.int64)\n",
        "    hid_state = char_rnn.initial_state(batch_size=1)\n",
        "    \n",
        "    #feed the seed phrase, if any\n",
        "    for i in range(len(seed_phrase) - 1):\n",
        "        print(x_sequence[:, -1].shape, hid_state.shape)\n",
        "        out, hid_state = char_rnn(x_sequence[:, i], hid_state)\n",
        "    \n",
        "    #start generating\n",
        "    for _ in range(max_length - len(seed_phrase)):\n",
        "        print(x_sequence.shape, x_sequence, hid_state.shape)\n",
        "        out, hid_state = char_rnn(x_sequence[:, -1], hid_state)\n",
        "        # Be really careful here with the model output\n",
        "        p_next = F.softmax(out / temperature, dim=-1).data.numpy()[0]\n",
        "        \n",
        "        # sample next token and push it back into x_sequence\n",
        "        print(p_next.shape, len(tokens))\n",
        "        next_ix = np.random.choice(len(tokens), p=p_next)\n",
        "        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
        "        print(x_sequence.shape, next_ix.shape)\n",
        "        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
        "        \n",
        "    return ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z5MsUYVRxuD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "2bd2bf9a-e07e-43c1-80ea-0ab5a2a49611"
      },
      "source": [
        "# An example of generated text.\n",
        "print(generate_sample(max_length=500, temperature=0.2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-4ddb4f95fa5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# An example of generated text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: generate_sample() missing 1 required positional argument: 'char_rnn'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux2O8g2BRxuE"
      },
      "source": [
        "### More poetic model\n",
        "\n",
        "Let's use LSTM instead of vanilla RNN and compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgsuFT0ZRxuE"
      },
      "source": [
        "Plot the loss function of the number of epochs. Does the final loss become better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "sOYIo1LpRxuF"
      },
      "source": [
        "# Your beautiful code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB5N4XySRxuF"
      },
      "source": [
        "Generate text using the trained net with different `temperature` parameter: `[0.1, 0.2, 0.5, 1.0, 2.0]`.\n",
        "\n",
        "Evaluate the results visually, try to interpret them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "TvhmEY6nRxuG"
      },
      "source": [
        "# Text generation with different temperature values here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPL14oaVRxuG"
      },
      "source": [
        "### Saving and loading models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh544JDnRxuH"
      },
      "source": [
        "Save the model to the disk, then load it and generate text. Examples are available [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html])."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Ha3Gzh5ERxuH"
      },
      "source": [
        "# Saving and loading code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFoG23j4RxuH"
      },
      "source": [
        "### References\n",
        "1. <a href='http://karpathy.github.io/2015/05/21/rnn-effectiveness/'> Andrew Karpathy blog post about RNN. </a> \n",
        "There are several examples of genration: Shakespeare texts, Latex formulas, Linux Sourse Code and children names.\n",
        "2. <a href='https://github.com/karpathy/char-rnn'> Repo with char-rnn code </a>\n",
        "3. Cool repo with PyTorch examples: [link](https://github.com/spro/practical-pytorch`)"
      ]
    }
  ]
}